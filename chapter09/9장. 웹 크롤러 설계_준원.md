# 9장 - 웹 크롤러 설계
- 웹 크롤러는 로봇 또는 스파이더라 부름
- 검색 엔진에 널리 쓰이는 기술
- 웹에 새로 올라오거나 갱신된 컨텐츠를 찾는 것이 주 목적
  - 콘텐츠란 웹 페이지, 이미지, 비디오, PDF 파일 등을 통틀어서 의미
- 검색 엔진 인덱싱
  - 크롤러의 가장 보편적인 용례
  - 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스를 생성
- 웹 아카이빙
  - 나중에 사용할 목적으로 장기 보관하기 위해 웹에서 정보를 모으는 절차
- 웹 마이닝
  - 유명 금융 기업들이 크롤러를 사용해 주주총회 자료나 연차 보고서 등을 받아 사용
- 웹 모니터링
  - 크롤러 사용 시 저작권이나 상표권이 침해되는 사례 모니터링 가능
- 크롤러의 복잡도는 처리할 데이터 규모에 따라 매우 달라짐.
  - 웹 크롤러가 감당해야 할 데이터의 규모와 기능을 알아내야 함

## 1단계 - 문제 이해 및 설계 범위 확정
- 웹 크롤러의 기본 알고리즘
  - URL 집합이 주어지면, URL이 가리키는 모든 웹 페이지를 다운로드
  - 다운 받은 웹 페이지에서 URL을 추출
  - 추출된 URL들을 다운로드할 URL 목록에 추가하고 위 과정을 처음부터 반복
- 요구 사항에 대한 질의 응답
  - Q) 크롤러의 주된 용도는? 검색 엔진 인덱스 생성, 데이터 마이닝 혹은 그 외의 용도?
    - A) 검색 엔진 인덱스 생성
  - Q) 매 달 얼마나 많은 웹 페이지를 수집해야 하는가?
    - A) 10억 개의 웹 페이지를 수집
  - Q) 새로 만들어진 웹 페이지나 수정된 웹 페이지도 고려하는가?
    - A) 넹
  - Q) 수집한 웹 페이지는 저장하는가?
    - A) 넹. 5년 저장함
  - Q) 중복된 컨텐츠는 어떡하는가?
    - A) 중복된 컨텐츠를 갖는 페이지는 무시해도 됨
- 주의를 기울여야 할 속성
  - 규모 확장성
    - 웹은 거대하므로 병행성(parallelism)을 활용하면 효과적인 웹 크롤링 가능
  - 안정성(robustness)
    - 웹은 함정으로 가득함.
    - 잘못 작성된 HTML서버, 무반응인 서버, 장애/악성 코드가 붙은 링크 등
    - 이런 함정과 비정상적 입력과 환경에 잘 대응해야 함
  - 예절
    - 수집 대상 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내서는 안됨
  - 확장성
    - 새로운 형태의 컨텐츠를 지원하기 쉬워야 함
    - 기존에 없던 이미지 파일 크롤링을 추가하려고 할 때 전체 시스템을 새로 설계하는 것온 옳지 않음.

### 개략적 규모 추정
- 매달 10억 개의 웹 페이지 다운
- QPS
  - 10억 / 30일 / 24시간 / 3600초 = 약 400페이지/s
- 최대 QPS
  - 2배 정도로 가정
  - 800
- 웹 페이지의 평균 크기는 500k라고 가정
  - 10억 * 500k = 500TB/월
- 5년간 저장
  - 500TB * 12개월 * 5년 = 30PB

## 2단계 - 개략적 설계안 제시 및 동의 구하기
### 시작 URL 집합
- 웹 크롤링의 출발점
- 크롤러가 가능한 많은 링크를 탐색할 수 있도록 하는 URL을 고르는 것이 바람직
- 일반적으로 전체 URL 공간을 작은 부분집합으로 나누는 전략을 사용
- 다른 방법은 주제별로 다른 시작 URL을 사용하는 것
  - 쇼핑, 스포츠, 건강 등

### 미수집 URL 저장소
- 크롤러는 크롤링 상태를 2가지로 관리함
  - 다운로드할 URL
    - 미수집 URL(URL frontier) 저장소라고 부름
      - FIFO Queue라고 생각하면 됨
  - 다운로드된 URL

### HTML 다운로더
- 웹 페이지를 다운로드하는 컴포넌트
- 다운로드 할 페이지는 URL frontier가 제공함

### 도메인 이름 변환기
- 웹 페이지를 다운 받으려면 URL을 IP주소로 변환하는 절차가 필요함
- HTML 다운로더는 도메인 이름 변환기를 사용해 URL에 대응되는 IP 주소를 알아냄

### 콘텐츠 파서
- 다운로드를 위해서는 파싱과 검증 절차를 거쳐야 함
  - 이상한 웹 페이즈는 문제 발생 소지가 있고, 저장 공간을 낭비함
- 크롤링 서버 안에 콘텐츠 파서를 구현 시 크롤링 과정이 느려질 수 있기에 분리.

### 중복 컨텐츠 확인
- 웹에 공개된 연구 결과에 따르면 29% 가량의 웹 페이지 컨텐츠는 중복이라고 함
- HTML 문서의 문자열은 비교하는 것은 비효율적임
  - 효과적인 방법은 웹 페이지 해시 값을 비교하는 것

### 컨텐츠 저장소
- HTML 문서를 보관하는 시스템
- 저장소 구현 시고려할 점
  - 데이터의 유형과 크기
  - 저장소 접근 빈도
  - 데이터의 유효 기간 등
- 디스크와 메모리를 동시에 사용할 예정
  - 대부분의 컨텐츠는 디스크에 저장
  - 인기 있는 컨첸트는 메모리에 둬 접근 지연시간을 감소

### URL 추출기
- HTML 페이지를 파싱해 링크를 추출

### URL 필터
- 특정 컨텐츠 타입이나 파일 확장자를 갖는 URL 접속 오류가 발생하는 URL 등 접근 제외 목록에 포함된 URL을 필터링

### 이미 방문한 URL 처리
- 이미 방문한 URL은 다시 방문하지 않아야 함
- 블룸 필터나 해시 테이블이 널리 사용됨

### URL 저장소
- 이미 방문한 URL을 저장하는 저장소

## 3단계 - 상세 설계
### DFS vs BFS
- 웹은 방향이 있느 ㄴ그래프와 같음
- 페이지 = 노드
- 하이퍼링크(URL) = 엣지
- DFS는 좋지 않을 가능성이 높음
  - 깊이가 어느정도 깊이까지 갈 지 모르기 때문임
- 따라서 웹 크롤러는 보통 BFS를 사용함.
  - BFS는 FIFO Queue를 사용함.
  - FIFO Queue를 사용했을 때의 문제점
    - 한 페이지에서 나오는 링크의 상당수는 같은 서버로 되돌아감.
    - 표준 BFS 알고리즘은 URL 간의 우선순위가 없음.
    - 하지만 실제로는 모든 웹 페이지의 중요성과 품질은 같지 않다.
    - 페이지 rank, 트래픽의 양, 엡데이트 빈도 등의 우선순위가 필요함.

### 미수집 URL 저장소
- **예의**
  - 수집 대상 서버에게 짧은 시간 안에 너무 많은 요청을 보내지 말아야 함.
  - 한 번에 한 페이지만 요청하는 것이 예의다.
  - 웹사이트의 호스트명과 다운로드를 수행하는 작업 스레드 사이의 관계를 유지해 만족할 수 있음.
    - 큐 라우터
      - 같은 호스트에 속한 URL은 언제나 같은 큐로 가도록 보장해줌
    - 매핑 테이블
      - 호스트 이름과 큐 사이의 관계를 보관하는 테이블
    - FIFO Queue
      - 같은 호스트에 속한 URL은 언제나 같은 큐에 보관
    - 큐 선택기
      - 큐들을 순회하면서 큐에서 URL을 꺼내서 해당 큐에서 나온 URL을 다운로드하도록 지정된 작업 스레드에 전달
    - 작업 스레드
      - 전달된 URL을 다운로드하는 작업 수행
      - 작업은 순차처리되며 작업 사이 일정한 지연시간을 둘 수 있다.
- **우선순위**
  - 같은 키워드가 뜨는 페이지라도 서로 다른 중요도를 갖고 있음
    - 페이지랭크, 트래픽 양, 갱신 빈도 등 다양한 척도를 사용할 수 있음.
  - 순위 결정 장치
    - URL을 입력 받아 우선순위를 계산
  - 큐
    - 우선 순위별로 큐가 하나씩 할당
  - 큐 선택기
    - 임의 큐에서 처리할 URL을 꺼내는 역할을 담당함.
- **신선도**
  - 페이지는 수시로 추가/삭제/변경이 일어남.
  - 주기적으로 재수집이 필요함.
    - 웹 페이지의 변경 이력 활용
    - 우선순위를 활용해 중요한 페이지는 더 자주 재수집
- **미수집 URL 저장소를 위한 지속성 저장장치**
  - 크롤러가 처리 할 수 많은 URL을 모두 메모리에 보관하는 것은 옳지 않음.
  - 전부 디스크에 저장하는 것도 성능 병목지점이 될 수 있음.
  - IO 비용을 줄이기 위해 메모리 버퍼에 큐를 두어 주기적으로 디스크에 기록하도록 함.

### HTML 다운로더
- **Robots.txt**
  - 로봇 제외 프로토콜이라고도 부름
  - 이 파일에는 크롤러가 수집해도 되는 페이지 목록이 있음.
  - 크롤러는 이 파일에 나열된 규칙을 먼저 확인해야 함.
- **성능 최적화**
  - **분산 크롤링**
    - 크롤링 작업을 여러 서버에 분산
    - URL 공간을 작은 단위로 분할해 각 서버가 일부를 담당하도록 함.
  - **도메인 이름 변환 결과 캐시**
    - DNS Resolver는 크롤러 성능의 병목 중 하나임
      - DNS 요청을 보내고 결과를 받는 작업의 동기적 특성 때문
    - 크롤러 스레드 가운에 하나라도 이 작업을 수행하면 다른 스레드의 DNS 요청은 전부 블록됨
  - **지역성**
    - 크롤링 작업을 지역별로 분산하는 방법
    - 지역적으로 가까우면 페이지 다운로드 시간이 줄어듦
    - 크롤링 서버, 캐시, 큐, 저장소 등 대부분 컴포넌트에 적용 가능
  - **짧은 타임아웃**
    - 웹 서버가 응답이 느리거나 아예 응답하지 않을 수 있음.
    - 최대 대기 시간을 설정해두어야 함.
- **안정성**
  - 안정 해시
    - 다운로더 서버들에 부하를 분산할 때 사용.
  - 크롤링 상태 및 수집 데이터 저장
    - 장애 발생 시에도 쉽게 복구 할 수 있도록 크롤링 상태와 수집된 데이터를 지속적으로 기록함
  - 예외 처리
    - 에러는 불가피하며 자주 발생함.
    - 예외가 발생해도 전체 시스템이 중단되는 일이 없어야 함.
  - 데이터 검증
    - 시스템 오류를 방지하기 위한 중요 수단 가운데 하나임
- **확장성**
  - 새로운 형태의 콘텐츠를 쉽게 지원할 수 있도록 해야함.
- **문제 있는 컨텐츠 감지 및 회피**
  - 중복 컨텐츠
    - 해시나 체크섬을 사용해 중복 컨텐츠를 탐지
  - 거미 덫(spider trap)
    - 크롤러를 무한 루프에 빠뜨리도록 설계한 웹 페이지임
    - 디렉토리 구조를 무한하게 만드는 등의 덫이 있을 수 있음.
    - 모든 종류의 덫을 피할 수 있는 방법은 없음.
  - 데이터 노이즈
    - 광고나 스크립트 코드, 스팸 같은 URL은 제외해야 함.

## 4단계 - 마무리
- 추가 논의 사항
  - 서버 측 렌더링
    - 많은 웹사이트가 JS나 Ajax 등으로 링크를 즉석에서 만듦
    - 웹 페이지를 그대로 다운 받으면 안됨
    - 서버 사이드 렌더링을 사용하면 해결 가능
  - 원치 않는 페이지 필터링
    - 크롤링에 소요되는 자원은 유한하므로 품질이 떨어지거나 스팸성인 페이지를 걸러낼 수 있도록 해야 함
  - DB 다중화 및 샤딩
    - 다중화와 샤딩 같은 기법을 통해 데이터 계층의 가용성과 규모 확장성, 안정성을 높임
  - 수평적 규모 확장성
    - 대규모 크롤링을 위해서는 수평적 확장이 필요할 수 있음
    - 이를 위해서는 서버가 상태 정보를 유지하지 않는 무상태 서버가 되도록 해야 함
  - 가용성, 일관성, 안정성
    - 성공적인 대형 시스템을 만들기 위해서는 필수로 고려해야할 것들임
  - 데이터 분석 솔루션
    - 데이터를 수집하고 분석하는 것은 매우 중요함

### Q) 큐가 직접적으로 작업 스레드에 연결되지 않고 큐 선택기가 존재하는 이유는?
- 가장 큰 이뉴는 큐와 작업 스레드가 1대1로 매핑되면 분산 환경을 구성한 이유가 없어지기 때문인 것 같음
  - 1:1로 매핑된다는 의미는 각각의 큐와 작업 스레드가 단일 장애지점이 될 수 있음
  - 작업 양에 따른 분산도 어려움
    - 작업이 많이 몰리는 큐와 작업 스레드의 부담을 덜어줘야하는 데 이를 수행할 수 없음
- 의존 관계가 생기게 됨.
  - 큐가 1대1로 매핑되게 되면 수평확장에서 불리해지게 됨.

### Q) 작업 스레드 선택기가 아니라 큐 선택기라고 하는 이유는?
- 주체가 '**작업 스레드**'이기 때문이라고 생각함
  - 작업 스레드가 능동적으로 큐 선택기에게 할당 받을 작업을 요청하는 것이 좋다고 생각했음.
    - 큐 선택기가 작업 스레드의 작업 상태를 알게 할 방법이 의존성이 너무 심하게 생기게 된다고 생각함.