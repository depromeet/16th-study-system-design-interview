# 웹 크롤러 설계  

웹 크롤러는 검색 엔진 인덱싱, 웹 아카이빙, 웹 마이닝, 웹 모니터링 등 여러 용도로 사용되며,  
대규모 분산 시스템으로 설계할 때 고려해야 할 요소가 매우 많습니다.

---

## 1. 개략적 설계

### 1.1. 기본 흐름

1. **시작 URL 집합**
    - 크롤링의 출발점 URL들을 미수집 URL 저장소에 저장

2. **미수집 URL 저장소**
    - 아직 다운로드하지 않은 URL들을 관리 (FIFO 큐 사용)

3. **HTML 다운로더**
    - 미수집 URL 저장소에서 URL 목록을 가져옴
    - **도메인 이름 변환기**를 사용하여 URL로부터 IP 주소를 알아내고 웹 페이지를 다운로드

4. **콘텐츠 파서**
    - 다운로드한 HTML 페이지를 파싱 및 검증
    - 중복 콘텐츠 여부 확인

5. **중복 URL/콘텐츠 체크**
    - 이미 저장소에 존재하는지 확인 후, 있으면 버림

6. **URL 추출 및 필터링**
    - HTML 페이지에서 링크를 추출
    - **URL 필터**를 통해 대상이 아닌 URL 제거
    - 남은 URL을 중복 URL 판별 단계로 전달

7. **URL 저장 및 미수집 URL 저장소 추가**
    - 새로운 URL은 URL 저장소에 저장하고, 미수집 URL 저장소에 추가

---

## 2. 주요 컴포넌트 및 용어 정리

- **시작 URL 집합**
    - 크롤링 시작점, 예를 들어 도메인별, 카테고리별 URL 집합

- **미수집 URL 저장소**
    - 다운로드할 URL들을 저장/관리 (FIFO 큐)

- **HTML 다운로더**
    - 인터넷에서 웹 페이지를 다운로드하는 컴포넌트
    - **도메인 이름 변환기**와 연계되어 DNS 룩업 수행

- **콘텐츠 파서**
    - HTML 파싱 및 검증 담당 (연산량이 많으므로 독립 컴포넌트로 분리)

- **콘텐츠 저장소**
    - 다운로드한 HTML 문서를 보관 (디스크, Hot 컨텐츠는 메모리)

- **URL 추출기**
    - HTML 페이지 내 링크를 추출 (상대경로 → 절대경로 변환)

- **URL 필터**
    - 크롤링 대상에서 제외할 URL (특정 콘텐츠 타입, 오류 발생 URL 등)

- **URL 저장소**
    - 이미 방문한 URL들을 저장 (블룸 필터, 해시 테이블 등 사용)

---

## 3. 상세 설계 및 알고리즘

### 3.1. 탐색 방식: DFS vs BFS

- **DFS (Depth-First Search)**
    - 깊이 우선 탐색, 그래프의 깊이를 예측하기 어렵고,  
      한쪽 도메인에 너무 깊게 파고들 가능성이 있음
    - 웹 크롤링에는 부적합

- **BFS (Breadth-First Search)**
    - 너비 우선 탐색, FIFO 큐를 사용하여 모든 URL을 공평하게 다룸
    - **문제점**:
        - 한 페이지에서 같은 도메인으로 많은 링크가 있을 경우,  
          해당 서버에 과도한 요청을 보낼 수 있음
        - 모든 페이지가 같은 중요도를 갖지 않으므로 우선순위 부여 필요

### 3.2. 우선순위 및 신선도 관리

- **우선순위(Prioritizer)**
    - 페이지 랭크, 트래픽 양, 갱신 빈도 등으로 URL의 중요도를 계산
    - 전면 큐(Front Queue)와 후면 큐(Back Queue)를 이용하여  
      우선순위 결정 및 예의(Politeness)를 보장

- **예의(Politeness)**
    - 동일 호스트에 대해서 한 번에 한 페이지만 요청
    - **큐 라우터**: 같은 호스트의 URL은 항상 같은 큐로
    - **매핑 테이블**: 호스트 이름과 큐의 관계 관리
    - **작업 스레드**: 각 큐에서 URL을 꺼내 다운로드 (지연시간 적용 가능)

- **신선도**
    - 중요 페이지는 자주 재수집 (업데이트 빈도 고려)
    - 재수집 전략: 웹 페이지 변경 이력, 우선순위 반영 등

---

## 4. 성능 최적화 및 확장성 고려

### 4.1. 분산 크롤링

- **여러 서버에 작업 분산**
    - 각 서버는 여러 스레드를 사용하여 크롤링 작업 수행
    - URL 공간을 분할하여 각 서버가 담당

### 4.2. 캐싱 및 DNS 최적화

- **도메인 이름 변환 캐시**
    - DNS 요청은 지연 요인이므로, 조회 결과(IP 주소)를 캐시에 보관
    - 주기적으로 갱신

### 4.3. 데이터 저장 및 관리

- **미수집 URL 저장소의 지속성**
    - 메모리와 디스크를 혼합하여 사용 (메모리 버퍼 후 주기적 디스크 저장)

- **콘텐츠 저장소**
    - 5년치 보관 등 대용량 데이터 저장(예: PB 단위)

- **URL 저장소**
    - 블룸 필터, 해시 테이블 등으로 중복 URL 관리

### 4.4. 안정성 및 예외 처리

- **안정 해시 (Consistent Hashing)**
    - 다운로더 서버 부하 분산 및 확장성 지원
- **크롤링 상태 및 수집 데이터의 지속적 기록**
    - 장애 복구 및 데이터 검증

---

## 5. 부가 기능 및 고려 사항

- **Robots.txt**
    - 크롤링 전 해당 파일을 확인하여 접근 가능한 페이지 선별
    - 주기적 캐시 갱신 필요

- **서버 측 렌더링**
    - JavaScript, AJAX로 동적으로 생성되는 링크 처리 (동적 렌더링 적용)

- **원치 않는 페이지 필터링**
    - 스팸, 품질이 낮은 페이지 필터링을 위한 추가 모듈 고려

- **데이터베이스 다중화 및 샤딩**
    - 수평적 규모 확장 (Scale Out) 및 무상태(stateless) 서버 구성

- **데이터 분석 솔루션**
    - 크롤링 데이터를 기반으로 트렌드 분석, 검색 엔진 인덱싱 등 활용

---

## 6. 개략적 규모 추정

- **매달 다운로드 페이지 수**:
    - 예: 10억 개 웹 페이지
    - QPS: `10억 / (30 * 24 * 3600) ≒ 400 page/sec`
    - Peak QPS: 최대 800 page/sec (2배 가정)

- **웹 페이지 평균 크기**:
    - 약 500 KB
    - 월별 저장 용량: `10억 * 500KB ≒ 500 TB`
    - 5년간 보관 시: `500 TB * 12 * 5 = 30 PB`

---

웹 크롤러는 단순한 아이디어(시작 URL → 다운로드 → 파싱 → 추출 → 재수집)에서 시작하지만,  
실제 대규모 환경에서는 **확장성, 안정성, 예의, 신선도** 등 다양한 문제를 함께 고려해야 합니다.  
효율적인 미수집 URL 관리, 적절한 우선순위 부여, 분산 처리, 캐시 최적화,  
그리고 장애 및 예외 상황에 대한 견고한 설계가 필수적입니다.

이상은 웹 크롤러 설계에 대한 내가 정리한 내용입니다.
