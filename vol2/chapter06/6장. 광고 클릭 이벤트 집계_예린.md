# 6장. 광고 클릭 이벤트 집계

## 목표

대규모 시스템에 걸맞는 광고 클릭 이벤트 집계 시스템을 설계하라.

- RTB(Real-Time Bidding): 디지털 광고의 핵심 프로세스
    - 해당 경매 절차를 통해 광고각 나갈 지면을 거래
    - 속도 중요: 1초 내에 모든 프로세스 마무리
    - 데이터 정확성 중요: 온라인 광고가 얼마나 효율적이었는지 측정하는 결정적인 역할을 하므로 광고주가 얼마나 많은 돈을 지불할지에 영향
- 온라인 광고 핵심 지표: CTR(클릭률), CVR(전환률)

## 1단계. 문제 이해 및 설계 범위 확정

- 입력 데이터 형태: 분산된 로그 파일, 클릭 이벤트(ad_id, click_time-stamp, user_id, ip, country) 수집마다 끝에 추가
- 데이터 양: 매일 10억 개 광고 클릭 발생, 2백만 회 광고 게재, 광고 클릭 이벤트 수는 매년 30%씩 증가
- 중요하게 지원해야 할 질의
    - 특정 광고에 대한 지난 M분간의 클릭 이벤트 수
    - 지난 1분간 가장 많이 클릭된 광고 100개, 질의 기간과 광고 수는 변경 가능해야 하며 집계는 매분 이루어짐
    - ip, user_id, country 등의 속성을 기준으로 상기 2개 질의 결과 필터링 가능
- 엣지 케이스 고려
    - 예상보다 늦게 도착하는 이벤트
    - 중복된 이벤트
    - 시스템 일부가 언제든지 다운 가능 -> 복구 고려
- 지연 시간 요건
    - 모든 처리는 수 분 내 처리
    - RTB와 광고 클릭 집계의 지연 시간 요건은 매우 다름
        - RTB 지연 시간은 1초 미만
        - 광고 클릭 이벤트 집계는 몇 분

### 기능 요구사항

- 지난 M분 동안의 ad_id 클릭 수 집계
- 매분 가장 많이 클릭된 상위 100개 광고 아이디 반환
- 다양한 속성에 따른 집계 필터링 지원
- 데이터 양은 페이스북이나 구글 규모

### 비기능 요구사항

- 집계 결과 정확성은 데이터가 RTB 및 광고 과금에 사용되므로 중요
- 지연되거나 중복된 이벤트에 대한 적절한 처리
- 견고성: 부분 장애 감내 가능
- 지연 시간 요구사항: 최대 수 분 넘지 않아야 함

### 개략적 추정

- DU 10억 명
- 각 사용자가 하루 평균 1개 광고 클릭 가정 -> 하루 10억 건의 광고 클릭 이벤트 발생
- 광고 클릭 QPS = 10^9이벤트 / 하루 10^5초 = 10,000
- 최대 광고 클릭 QPS = 평균 QPS * 5 = 50,000 QPS
- 광고 클릭 이벤트 하나당 0.1KB 저장 용량 필요 가정 -> 일일 저장소 요구량 = 0.1KB * 10억 = 100휴
- 월간 저장 용량 요구량 = 3TB

## 2단계. 개략적 설계안 제시 및 동의 구하기

### 질의 API 설계

- 목적: 서버 간 통신 규약 생성
- 클라이언트: 대시보드 이용하는 데이터 과학자, 제품 관리자, 광고주 등
- 질의 발생 시점: 대시보드 사용 순간
- 기능 요구사항
    - 다양한 속성 기준으로 집계 결과 필터링 기능 지원
    - M분 간 ad_id에 발생한 클릭 수 집계
        - `GET /v1/ads/{:ad_id}/aggregated_count?from={from}&to={to}&filter={filter}`
        - response: `ad_id`, `count`
    - M분 간 가장 많은 클릭 발생한 상위 N개 ad_id 목록 반환
        - `GET /v1/ads/popular_ads?count={count}&window={window}&filter={filter}`
        - reponse: `ad_ids`

### 데이터 모델

- 원시 데이터
    - 로그 파일
    - 여러 애플리케이션 서버에 산재되어 있음
- 집계 데이터
    - 필터링에 따라 데이터를 집계한 결과
    - 지난 M분 간 가장 많이 클릭된 상위 N개 광고 반환 질의 지원 위해서는 `window_size`, `update_time_minute`, `most_clicked_ads` 관리
- 비교
    - 원시 데이터
        - 장점: 데이터 손실 없음, 필터링 및 재계산 지원
        - 단점: 막대한 데이터 용량, 낮은 질의 성능
    - 집계 결과
        - 장점: 데이터 용량 절감, 빠른 질의
        - 단점: 데이터 손실
- 결론: **둘 다 저장하자**
    - 원본 데이터
        - 문제 발생 시 디버깅에 활용
        - 백업 데이터로 활용
        - 오래된 데이터는 냉동 저장소로 이동
        - 버그로 집계 데이터 손상 시 버그 수정 후 원시 데이터에서 집계 결과 재생성
    - 집계 데이터
        - 질의 시 활용
        - 활성 데이터 역할
        - 질의 성능 높이기 위해 튜닝

### 올바른 데이터베이스의 선택

- 고려 사항
    - 데이터는 어떤 모습인가? 관계형 데이터인가? 문서 데이터인가? 이진 대형 객체인가?
    - 작업 흐름이 읽기 중심인가 쓰기 중심인가? 아니면 둘 다인가?
    - 트랜잭션을 지원해야 하는가?
    - 질의 과정에서 SUM이나 COUNT 같은 온라인 분석 처리(OLAP) 함수를 많이 사용해야 하는가?

#### 원시 데이터

- 평균 쓰기 QPS = 10,000 -> 쓰기 중심 시스템
- 백업, 재계산 용도 -> 읽기 빈도 낮음
- 쓰기 및 시간 범위 질의에 최적화된 카산드라나 InfluxDB 사용이 바람직
- ORC,파케이, AVRO 같은 칼럼형 데이터 형식을 사용하여 S3에 저장하는 방법도 있음

#### 집계 데이터

- 시계열 데이터
- 쓰기 & 읽기 둘 다 많이 사용
- 매 분마다 DB에 질의를 던져 고객에게 최신 집계 결과 제시해야하므로 읽기 연산 많이 발생
- 매 분마다 집계 및 기록으로 인한 쓰기 작업도 매우 빈번
- 따라서, 원시 데이터와 같은 유형의 데이터베이스 활용하는 것이 가능

### 개략적 설계안

- 입력: 원시 데이터 (무제한 데이터 스트림)
- 출력: 집계 결과

<img alt="image" src="https://github.com/user-attachments/assets/2a461602-12cc-430b-95e9-5ce8b29242dc">

#### 비동기 처리

- 동기식 처리
- 생산자와 소비자 용량이 항상 같을 수 없으므로 좋지 않음
- 트리팩 급증으로 인한 이벤트 수가 소비자의 처리 용량을 훨씬 넘어서는 경우, 소비자는 메모리 부족 오류 등의 예기치 않은 문제 경험 가능
- 특정 컴포넌트의 장애가 전체 시스템의 장애로 이어짐
- 해결 방법: 메시지 큐(카프카 등)을 도입하여 강한 결합을 느슨하게, 생산자와 소비자 규모 확장을 독립적으로

<img alt="image" src="https://github.com/user-attachments/assets/f3703692-fcee-4448-8ffd-9e69c68db2b6">

### 집계 서비스

- 맵리듀스 프레임워크 사용
- DAG 모델이 적합
    - 시스템을 맵/집계/리듀스 노드 등의 작은 컴퓨팅 단위로 세분화
    - 각 노드는 한 가지 작업만 처리, 처리 결과는 다음 노드에 인계
- DAG 모델
    - 빅데이터를 입력으로 받아 병렬 분산 컴퓨팅 자원을 활용하여 빅데이터를 작은 크기 데이터로 변환할 수 있도록 설계된 모델
- 맵 노드
    - 데이터 출처에서 읽은 데이터를 필터링/변환
    - 맵 노드가 필수인가? 카프카 파티션이나 태그를 구성한 후 집계 노드가 카프카를 직접 구독하도록 하면 안되나?
        - 입력 데이터를 정리/정규화해야 하는 경우에 필요 -> ?
        - 데이터 생성 방식에 대한 제어권이 없는 경우 동일한 ad_id를 갖는 이벤트가 서로 다른 카프카 파티션에 입력될 수 있음
- 집계 노드
    - ad_id별 광고 클릭 이벤트 수를 매 분 메모리에서 집계
    - 맵리듀스 패러다임의 집계 노드는 사실상 리듀스 프로세스의 일부
- 리듀스 노드
    - 모든 집계 노드가 산출한 결과를 최종 결과로 축약
    - 각 집계 노드 관점에서 가장 클릭 많은 광고 추려서 리듀스 노드로 전송
- 주요 사용 사례
    1. 클릭 이벤트 수 집계

    - 맵 노드는 시스템에 입력되는 이벤트를 분배하고, 그 결과를 각 집계 노드가 집계

    2. 가장 많이 클릭된 상위 N개 광고 반환

    - 각 집계 노드는 힙을 내부적으로 사용하여 상위 3개 광고를 효율적으로 식별
    - 리듀스 노드는 전달 받은 9개 광고 중 1분간 가장 많이 클릭된 광고 찾아냄

    3. 데이터 필터링

    - 스타 스키마
        - 필터링 기준을 사전에 정의한 다음 해당 기준에 따라 집계
        - 데이터 웨어하우스에서 널리 쓰이는 기법
        - 차원: 필터링에 사용되는 필드
    - 장점
        - 이해 쉬움, 구축 간단
        - 기존 집계 서비스 재사용하여 스타 스키마에 더 많은 차원 생성 가능
        - 다른 추가 컴포넌트 필요 없음
        - 결과를 미리 계산해두는 방식으로, 필터링 기준에 따라 데이터를 빠르게 접근
    - 한계
        - 많은 버킷/레코드 생성

## 3단계. 상세 설계

- 스트리밍 vs 일괄 처리
- 시간과 집계 윈도
- 전달 보증
- 시스템의 규모 확장
- 데이터 모니터링 및 정확성
- 최종 설계 다이어그램
- 결함 내성

### 스트리밍 vs 일괄 처리

- 개략적 설계안 -> 일종의 스트림 처리 시스템
- 스트림 처리와 일괄 처리 방식을 모두 사용하자
    - 스트림 처리: 데이터 오는대로 처리, 실시간으로 집계된 결과를 생성
    - 일괄 처리: 이력 데이터를 백업
    - 동시 지원 시스템 아키텍처: 람다

|          | 서비스(온라인 시스템)   | 일괄 처리 시스템(오프라인 시스템)      | 스트리밍 시스템(실시간에 가깝게 처리하는 시스템) |
|----------|----------------|--------------------------|-----------------------------|
| 응답성      | 클라이언트에게 빠르게 응답 | 클라이언트에게 응답 X             | 클라이언트에게 응답 X                |
| 입력       | 사용자의 요청        | 유한한 크기를 갖는 입력, 큰 규모의 데이터 | 입력에 경계가 없음 (무한 스트림)         |
| 출력       | 클라이언트에 대한 응답   | 구체화 뷰, 집계 결과 지표 등        | 구체화 뷰, 집계 결과 지표 등           |
| 성능 측정 기준 | 가용성, 지연 시간     | 처리량                      | 처리량, 지연 시간                  |
| 사례       | 온라인 쇼핑         | 맵리듀스                     | 플링크                         |

- 람다 아키텍처
    - 단점: 두 가지 처리 경로를 지원하므로 유지 관리해야 할 코드가 두 벌
- 카파 아키텍처 ✅
    - 일괄 처리와 스트리밍 처리 경로를 하나로 결합하여 람다 아키텍처의 단점 극복
    - 단일 스트림 처리 엔진을 사용하여 실시간 처리, 끊임없는 데이터 재처리 문제 모두 해결

#### 데이터 재계산

<img alt="image" src="https://github.com/user-attachments/assets/ece3d6c4-32b6-44a0-9442-aca6f84f79a5">

- 버그 발생 시 해당 시점부터 원시 데이터 다시 읽어 재계산해야함
- 동작 흐름
    1. 재계산 서비스는 원시 데이터 저장소에서 데이터 검색. 이로갈 처리 프로세스 따름
    2. 추출된 데이터는 전용 집계 서비스로 전송
        - 전용 집계 서비스: 실시간 데이터 처리 과정이 과거 데이터 재처리 프로세서와 간섭하는 일 방지
    3. 집계 결과는 두번째 메시지 큐로 전송되어 집계 결과 데이터베이스에 반영

### 시간

- 이벤트 시각: 광고 클릭 발생 시각
- 처리 시각: 집계 서버가 클릭 이벤트를 처리한 시스템 시각
- 네트워크 지연이나 비동기적 처리로 인한 이벤트 시각과 처리 시각 격차 발생
- 이벤트 시각 ✅ (**데이터 정확도 중요**)
    - 장점: 광고 클릭 시점을 정확하게 아는 것은 클라이언트 -> 집계 경로가 보다 정확
    - 단점: 클라이언트가 생성한 타임스템프에 의존하므로 클라이언트 시각이 잘못 설정되었거나, 악의적인 조작 문제 발생 가능
- 처리 시각
    - 장점: 서버 타임스탬프가 클라이언트 타임스탬프보다 안정적
    - 단점: 이벤트 시스템에 도착 지연으로 집계 결과 부정확함

#### 지연된 이벤트 처리

- 워터마크 기술 사용
    - 이벤트 발생 시각 기준으로 속할 윈도우 결정되므로 도착 지연 시 집계되지 못하는 문제 발생
    - 워터마크: 각 윈도우 마지막에 붙은 여분의 사각형 (집계 윈도우의 확장)
    - 크기: 비즈니스 요구사항에 따라 조정
        - 짧을수록 정확도는 떨어지지만 시스템 응답 지연은 감소
    - 장점: 데이터 정확도 증가
    - 단점: 지연 시간 증가
- 그럼에도 아주 늦게 도착하는 이벤트는 처리 못함
    - 발생할 확률이 낮은 이벤트 처리를 위해 시스템을 복잡하게 설계하면 투자대비 효능 저하
    - 사소한 데이터 오류는 하루치 데이터 마감 시 조정 가능

### 집계 윈도우

- 종류
    - 텀블링(고정) 윈도우
        - 시간을 같은 크기의 겹치지 않는 구간으로 분할
        - 매 분 발생하는 클릭 이벤트 집계에 적합
    - 호핑 윈도우
    - 슬라이딩 윈도우
        - 스트림을 슬라이딩하며 같은 시간 구간 내 이벤트 집계 (구간 겹침)
        - M분간 가장 많이 클릭된 상위 N개 광고 파악에 적합
    - 세션 윈도우

### 전달 보장

- 데이터 정확성과 무결성 중요
    - 집계 결과를 과금 등에 활용
- 고려 사항
    - 이벤트 중복 처리를 어떻게 피할 수 있는가?
    - 모든 이벤트의 처리를 어떻게 보장할 수 있는가?
- 카프카(메시지 큐): at-most once, at-least once, exactly once

#### 어떤 전달 방식을 택할 것인가

- exactly once
    - 데이터 몇 퍼센트 차이가 수백만 달러 차이로 이어짐
    - 참고: Yelp
- 데이터 중복 제거
    - 클라이언트 측: 같은 이벤트를 여러번 전송하는 경우
        - 광고 사기/위험 제어 컴포넌트 활용
    - 서버 장애: 집계 도중 집계 서비스 노드에 장애 발생하여 업스트림 서비스(카프카)가 이벤트 메시지에 응답 못하고, 같은 이벤트가 다시 전송되어 재차 집계
        - 업스트림 카프카에 소비된 데이터 반영이 안되어 집계 서비스가 복구되었을 때 이미 소비한 데이터를 다시 소비하려 할 것
        - 해결책: HDFS, S3 같은 외부 파일 저장소에 오프셋 기록 (업스트림 - 집계 서비스 노드 - HDFS/S3 - 다운스트림)
    - 만약, 외부 저장소에 저장한 후에 집계 서비스에 장애가 발생해서 결과를 전송하지 못했다면?
        - 이미 외부 저장소에 저장된 데이터는 다시 처리 시도 X
        - 해결책: 다운스트림에서 집계 결과 수신 확인 응답 받은 후에 외부 저장소에 오프셋 저장
    - 만약, 오프셋 저장 전에 집계 서비스 노드에 장애가 발생한다면?
        - 집계 결과 중복 전송
        - 해결책: (집계 결과 전송 ~ 수신 확인 응답 전송)을 분산 트랜잭션으로

### 시스템 규모 확장

- 사업은 매년 30% 성장, 트래픽은 3년마다 2배 성장

#### 메시지 큐 규모 확장

- producer: 생산자 인스턴스 수에는 제한을 두지 않으므로 확장성 쉽게 달성
- consumer: 소비자 그룹 내 재조정 메커니즘은 노드 추가/삭제를 통해 규모를 쉽게 조정할 수 있도록 설계
    - 시스템에 수백 개 카프카 소비자가 있는 경우에는 재조정 작업 시간이 길어져 수 분 이상 걸릴 수 있음
    - 유입 적은 시간대에 작업하여 영향 최소화

#### 브로커

- 해시 키
    - ad_id를 해시키로: 같은 id 갖는 이벤트는 같은 카프카 파티션에 저장되도록
- 하티션 수
    - 같은 ad_id 갖는 이벤트가 다른 파티션에 기록될 수 있음
    - 사전에 충분한 파티션 확보하여 프로덕션 환경에서 파티션 수가 동적으로 늘어나는 일 방지
- 토픽 물리적 샤딩
    - 지역/사업 유형 등에 따라 토픽을 물리적으로 샤딩
    - 장점: 시스템 처리 대여폭 증가, 단일 토픽에 대한 소비자 수 감소하여 소비자 그룹 재조정 시간 단축
    - 단점: 복잡도 증가, 유지 관리 비용 증가

#### 집계 서비스 규모 확장

- 노드 추가/삭제로 수평적 확장
- 집계 서비스 처리 대여폭 높이기
    1. ad_id마다 별도의 처리 스레드 배치 -> 구현 상대적 쉬움
    2. 집계 서비스 노드를 아파치 하둡 YARN 같은 자원 공급자에 배포 (디중 프로세싱 활용) -> 상대적으로 보편적으로 사용

#### 데이터베이스의 규모 확장

- 카산드라: 안정해시와 유사한 방식
- 데이터는 각 노드에 균등 분산
- 사본도 적당한 수만큼 만들어서 분산
- 각 노드는 해시 링 위 특정 해시 값 구간의 데이터 보관, 다른 가상 노드의 사본도 보관
- 클러스터링에 새 노드 추가 시 가상 노드 간 균형은 자동으로 다시 조정, 샤딩을 조정하는 과정 불필요

### 핫스팟 문제

- 핫스팟 문제로 인한 서버 과부하 문제
- 더 많은 집계 서비스 노드를 할당하여 완화
- 동작 흐름
    1. 특정 집계 서비스 노드에 과부하 발생 시 자원 관리자에게 추가 자원 신청
    2. 자원 관리자는 해당 서비스 노드에 과부하가 걸리지 않도록, 추가 자원 할당 (2개 추가 할당 가정)
    3. 원래 집계 서비스 노드는 각 서비스 노드가 100개씩 이벤트 처리할 수 있도록 이벤틀를 세 그룹으로 분할
    4. 집계 후 축약된 결과를 원래 집계 서비스 노드에 기록
- 전역-지역 집계, 분할 고유 집계 등의 방안도 존재

### 결함 내성

- 집계는 메모리에서 이루어지므로 집계 노드 장애 시 결과도 손실
    - 업스트림 카프카 브로커에서 이벤트를 다시 받아와서 집계
- 카프카 데이터를 원접부터 다시 재생하여 집계하면 오래 걸림
    - 업스트림 오프셋 같은 '시스템 상태'를 스냅샷으로 저장하고, 마지막으로 저장된 상태부터 복구
- 점진적 스냅샷을 할용하여 집계 서비스 복구 절차를 단순화

### 데이터 모니터링 및 정확성

- 집계 결과는 Real-Time Bidding 및 청구서 발행 목적으로 사용
- 정상 동작 모니터링 및 데이터 정확성 보장 중요

#### 지속적인 모니터링

- 지연 시간: 데이터 처리 각 단계마다 지연 시간이 추가될 수 있으므로, 시스템 중요 부분마다 시각 추적 가능하도록 해야 함
    - 기록 시각 사이의 차이를 지연 시간 지표로 변환해서 모니터링
- 메시지 큐 크기: 큐의 크기가 갑자기 늘어난다면 더 많은 집계 서비스 노드 추가 필요할 수 있음
    - 카프카는 분산 커밋 로그 형태로 구현된 메시지 큐
    - 카프카 사용할 경우 레코드 처리 지연 지표를 대신 추적
- 집계 노드의 시스템 자원: CPU, 디스크, JVM 같은 것에 관련된 지표

#### 조정

- 다양한 데이터를 비교하여 데이터 무결성 보증하는 기법
- 광고 클릭 집계 결과는 비교할 제 3자가 없음
    - 대안: 매일 각 파티션에 기록된 클릭 이벤트를 이벤트 발생 시각에 따라 정렬한 결과를 일괄 처리하여 만들어 낸 다음, 실시간 집계 결과와 비교

### 대안적 설계안

- 사고 프로세스를 설명하고 타협적 선택지 사이의 장단점을 설명하는 능력을 보여라
- 대안
    - 광고 클릭 데이터를 하이브에 저장 후 빠른 질의는 ElasticSearch 계층을 얹어서 처리
    - 집계는 클릭하우스나 드루이드 같느 OLAP 데이터베이스로 처리

## 4단계. 마무리
