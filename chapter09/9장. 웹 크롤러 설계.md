# **웹 크롤러 설계**

## **1. 문제 이해 및 설계 범위 확정**
### **웹 크롤러의 역할**
- **검색 엔진 인덱싱**: 웹 페이지를 모아 검색 엔진의 로컬 인덱스를 구축
- **웹 아카이빙**: 장기 보관을 위한 웹 콘텐츠 수집
- **웹 마이닝**: 유용한 데이터 분석을 위해 웹에서 정보 수집
- **웹 모니터링**: 저작권 및 상표권 침해 감지

### **요구 사항**
- 매달 **10억 개의 웹 페이지** 수집
- **수정 및 신규 페이지 포함**
- 수집한 데이터를 **5년간 저장**
- **중복 콘텐츠 제거 필요**

### **규모 추정**
- 초당 **400페이지 다운로드 (최대 800페이지)**
- 평균 웹 페이지 크기: **500KB**
- 월별 저장 용량: **500TB**
- 5년간 총 저장 용량: **30PB**

---

## **2. 개략적 설계**
### **웹 크롤러의 기본 작동 방식**
1. URL 목록을 입력받아 해당 웹 페이지 다운로드
2. 다운로드한 웹 페이지에서 새로운 URL 추출
3. 새로운 URL을 다시 다운로드 대상 목록에 추가하고 반복

### **설계 개요**
- **시작 URL 집합**: 크롤링을 시작할 초기 URL 목록
- **미수집 URL 저장소**: 다운로드할 URL을 저장하는 큐 (FIFO 구조)
- **HTML 다운로드**: 웹 페이지를 가져오는 모듈
- **도메인 이름 변환기 (DNS Resolver)**: URL을 IP 주소로 변환
- **콘텐츠 파서**: HTML을 파싱하여 유효성 검증
- **중복 콘텐츠 필터**: 같은 콘텐츠가 여러 번 저장되는 것을 방지
- **URL 추출기**: HTML에서 하이퍼링크 추출
- **URL 필터**: 특정 확장자나 오류가 있는 URL 제외
- **방문한 URL 저장소**: 이미 크롤링한 URL을 기록하여 중복 방지

### **작업 흐름**
1. 시작 URL을 **미수집 URL 저장소**에 저장
2. **HTML 다운로드 모듈**이 미수집 URL을 가져와 다운로드
3. **도메인 이름 변환기**가 URL을 IP 주소로 변환
4. **콘텐츠 파서**가 HTML을 파싱하여 유효성 확인
5. **중복 콘텐츠 필터**가 중복 페이지 제거
6. **URL 추출기**가 새 링크를 수집하여 필터링
7. **방문한 URL 저장소**와 비교하여 새로운 URL만 미수집 URL 저장소에 추가
8. 프로세스를 반복

---

## **3. 상세 설계**
### **탐색 알고리즘 선택 (DFS vs BFS)**
- **DFS (Depth-First Search)**: 너무 깊이 탐색할 위험이 있어 적합하지 않음
- **BFS (Breadth-First Search)**: 크롤링 시 **FIFO 큐**를 활용하여 더 적절

### **미수집 URL 저장소**
- URL 큐를 **도메인별 분할**하여 같은 서버로의 과부하 방지
- **우선순위 큐 적용**: 페이지 중요도(PageRank, 업데이트 빈도 등)에 따라 URL 크롤링 순서 조정

### **HTML 다운로드 최적화**
- **분산 크롤링**: 여러 서버에서 동시에 다운로드 수행
- **DNS 결과 캐싱**: DNS 조회 시간을 줄여 성능 최적화
- **로컬 크롤링 서버 활용**: 지역별 크롤링 서버를 분산 배치하여 지연 시간 단축
- **짧은 타임아웃 설정**: 응답이 느린 서버는 일정 시간 후 크롤링 중단

### **안정성 확보**
- **Consistent Hashing**: 크롤링 서버를 추가 및 제거할 때 균등한 부하 분산 보장
- **크롤링 상태 저장**: 장애 발생 시 중단된 지점에서 재시작 가능하도록 크롤링 진행 상태 기록
- **예외 처리 및 데이터 검증**: 에러 발생 시 전체 시스템이 중단되지 않도록 설계

### **확장성 고려**
- **플러그인 모듈 방식 적용**: 새로운 콘텐츠 유형 (예: 이미지, 동영상 등) 추가 지원 가능
- **URL 필터링 강화**: 스팸, 광고 페이지, 저작권 위반 페이지 등의 비효율적인 크롤링 방지

### **문제 해결 전략**
- **중복 콘텐츠 제거**: 페이지 해시값 비교를 활용하여 동일 콘텐츠 검출
- **스파이더 트랩 방지**: 무한 루프 방지를 위해 URL 길이 제한 및 특정 패턴 감지
- **데이터 노이즈 제거**: 광고, 스팸, 무의미한 스크립트 등을 필터링

---

## **4. 결론**
웹 크롤러를 설계할 때 고려해야 할 중요한 요소는 **확장성, 안정성, 예절(Politeness), 확장성(Evolvability)** 이다.  
이 설계안은 크롤링 성능을 극대화하면서도 서버 과부하를 방지하고, 재방문 전략을 통해 최신 데이터를 유지하는 데 중점을 둔다.

추가로 고려할 수 있는 요소:
- **서버 사이드 렌더링**: AJAX 및 JavaScript 기반 콘텐츠를 크롤링하기 위한 동적 렌더링 적용
- **스팸 필터링**: 저장 공간 절약 및 크롤링 품질 향상을 위한 스팸 탐지 알고리즘 추가
- **데이터베이스 샤딩**: 크롤링된 데이터 저장소의 확장성을 확보하기 위한 다중화 적용
- **수평적 확장**: 서버를 추가할 수 있는 Stateless 구조를 유지하여 대규모 크롤링을 지원
- **데이터 분석 솔루션**: 크롤링 데이터를 분석하여 시스템 성능 최적화 및 검색 품질 향상

이제 기본적인 웹 크롤러 설계가 완성되었으며, 실제 구현에서는 추가적인 세부 조정이 필요할 수 있다.
