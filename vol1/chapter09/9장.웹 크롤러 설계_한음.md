## 9장 웹 크롤러 설계

### 개요
웹 크롤러 : 검색 엔진에서 널리 쓰는 기술로, 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주된 목적

- 검색 엔진 인덱싱 : 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스를 생성 (ex. Googlebot)
- 웹 아카이빙 : 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차
- 웹 마이닝 : 웹의 폭발적 성장세 => 인터넷에서 유용한 지식을 도출 가능
- 웹 모니터링 : 저작권이나 상표권이 침해되는 사례 모니터링 (ex. Digimarc)

웹 크롤러의 복잡도는 웹 크롤러가 처리해야 하는 데이터의 규모에 따라 달라짐
=> 따라서 설계할 때 감당해야 하는 데이터의 규모와 기능 알아내는 것 중요함

### 웹 크롤러의 기본 알고리즘
1. URL 집합이 입력으로 주어지면, URL들이 가리키는 모든 웹 페이지를 다운로드
2. 다운받은 웹 페이지에서 URL들을 추출
3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 위 과정 반복

### 좋은 웹 크롤러가 만족시켜야할 속성들
- 규모 확장성 : 웹은 거대하다. 병행성을 활용하면 효과적인 크롤링 가능
- 안정성 : 웹은 함정으로 가득하다. 반응없는 서버, 장애, 악성 코드 링크 등에 대응할 수 있어야함.
- 예절 : 짧은 시간 동안 너무 많은 요청 보내서는 안됨
- 확장성 : 새로운 형태의 콘텐츠를 지원하기가 쉬어야함. (ex. 만약 이미지도 크롤링 하고 싶다면?)

### 개략적 규모 추정
- 매달 10억개 웹 페이지 다운로드
- QPS = 10억/30일/24시간/3600초 = 대략 400페이지/초
- Peak QPS = 2*QPS = 800
- 웹 페이지 크기 평균 500k라고 가정
- 1개월치 데이터를 보관하는데 500TB, 5년간은 30PB

### 시작 URL 집합
- 어떤 대학 웹사이트로부터 찾아 나가는 모든 웹페이지를 크롤링 하려면 => 해당 대학의 도메인 이름이 붙은 모든 페이지의 URL을 시작 URL로
- 전체 웹 크롤링한다면 좀 더 창의적이게 => 전체 URL 공간을 작은 부분집합으로 나누자. 정답은 없다.

### 미수집 URL 저장소
- 다운로드할 URL을 미수집 URL 저장소라고 함
- FIFO 큐라고 생각하자

### 중복 콘텐츠
- 웹에서 29% 가량의 콘텐츠는 중복이다
- HTML 문서를 문자열로 비교하면 너무 느림 => 웹 페이지의 해시 값을 비교하자

### 콘텐츠 저장소
- 데이터가 너무 많다 => 대부분의 콘텐츠는 디스크에 저장
- 인기 있는 콘텐츠는 메모리에 두어 접근 지연시간을 줄이자

### 웹 크롤러 작업 흐름
1. 시작 URL들을 미수집 URL 저장소에 저장
2. HTML 다운로더는 미수집 URL 저장소에서 URL 목록 가져옴
3. HTML 다운로더는 도메인 이름 변환기를 사용하여 IP 주소 알아내고 접속, 다운로드한다
4. 콘텐츠 파서는 다운된 HTML 페이지를 파싱해 올바른 형식을 갖춘 페이지인지 검증
5. 중복 컨텐츠인지 확인
6. 이미 저장소에 있으면 버린다. 없는 콘텐트는 저장한뒤 URL 추출기로 전달
7. URL 추출기는 해당 HTML 페이지에서 링크를 골라낸다
8. URL 필터로 전달
9. 필터링 된 URL을 중복 판별 단계로 전달
10. 저장소에 있는 URL은 버린다
11. 저장소에 없는 URL은 저장소에 저장 + 미수집 URL 저장소에 전달

### DFS vs BFS
- 웹은 유향 그래프다
- DFS는 좋은 선택이 아닌데, 어느 정도로 깊숙히 가게 될지 모르니까
- 웹 크롤러는 BFS를 사용한다. 근데 한가지 문제점이 있다. 한페이지에서 나오는 링크의 상당수는 같은 서버로 되돌아간다. (ex. wikipedia.com에서 추출한 링크는 대부분 내부링크, 이를 병렬로 처리하게 되면 위키피디아 서버 과부하 => 예의 없는 크롤러로 간주됨)

### 예의
- 짧은 시간안에 너무 많은 요청을 보내면 무례한 크롤러, DoS공격으로 간주되기도 함
- 따라서 동일 웹 사이트에 대해서는 한 번에 한 페이지만 요청하자
- 큐 라우터 사용하여 같은 호스트에 속한 URL은 같은 큐로 보내어 해결하자
- 큐 선택기로 큐들을 순회하면서 작업 스레드에 전달

### HTML 다운로더 설계
- Robots.txt에 나열된 규칙을 확인해, 수집해도 되는 페이지 목록을 파악하자
- 분산 크롤링을 수행하자 (지역 별로 분산하기도 함)
- 도메인 이름 변환기가 병목 지점, DNS 조회 결과를 캐시에 보관해 놓자
- 짧은 타임아웃, 이 시간동안 응답하지 않으면 넘어간다
- 안정 해시 기술을 사용해 다운로더 서버를 쉽게 추가하고 삭제하자
- 예외가 발생해도 우아하게 작업을 이어나가도록 예외처리를 잘하자
- 해시나 체크섬을 사용해 중복 콘텐츠를 쉽게 탐지하게 하자
- 거미 덫은 무한히 깊은 디렉터리 구조로 크롤러를 무한 루프에 빠뜨린다. URL의 최대 길이를 제한하자.
- 광고, 스크립트 코드, 스팸 URL 같은 데이터 노이즈는 거른다.